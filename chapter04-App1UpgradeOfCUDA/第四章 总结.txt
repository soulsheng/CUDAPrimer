沿用 第三章源码 


Step01-Parallel			第一步-程序的并行化


Step02-Coalesce			第二步-内存的存取模式


Step03-ParallelFurther		第三步-更多的并行化
理论上 256 个 threads 最多只能隐藏 256 cycles 的 latency。但是 GPU 存取 global memory 时的 latency 可能高达 500 cycles 以上。如果增加 thread 数目，就可以看到更好的效率。

Step04-syncthreads		第四步-Thread 的同步
在 CPU 上执行的部份，需要的时间加长了（因为 CPU 现在需要加总 8192 个数字）。为了避免这个问题，我们可以让每个block把自己的每个thread的计算结果进行加总。由于在 GPU 上多做了一些动作，所以它的效率会比较差一些。


Step05-UpgradeFurther		第五步-进一步改善
效率会变差的一个原因是，在这一版的程序中，最后加总的工作，只由每个 block 的 thread 0 来进行，但这并不是最有效率的方法。理论上，把 256 个数字加总的动作，是可以并行化的。最常见的方法，是透过树状的加法。
上一个版本的树状加法是一般的写法，但是它在 GPU 上执行的时候，会有 share memory 的 bank conflict 的问题（详情在后面介绍 GPU 架构时会提到）
如果还要再提高效率，可以把树状加法整个展开。

总结：

本人本实验：
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
				描述		1M数据耗费的时钟周期数		相对上一步提升倍数
优化之前					670M
Step01-Parallel-thread		线程并行	10M				67
Step02-Coalesce-global-memory	显存合并访问	2.5M				1
Step03-Parallel-block		块并行			0.1226M			20
Step04-syncthreads-tree		树状加法		0.1273M			0.96
Step05-UpgradeFurther-tree	conflict 展开循环	0.1260M			1.01	

峰值：Step03-Parallel-block 累积提升：670/0.1226 = 5464

原作者实验：
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
				描述		1M数据耗费的时钟周期数		相对上一步提升倍数
优化之前					640M
Step01-Parallel-thread		线程并行	8.3M				77
Step02-Coalesce-global-memory	显存合并访问	2.6M				3
Step03-Parallel-block		块并行			0.1465M			17
Step04-syncthreads-tree		树状加法		0.1600M			0.92
Step05-UpgradeFurther-tree	conflict 展开循环	0.1328M			1.20	（峰值）

峰值：SStep05-UpgradeFurther-tree 累积提升：640/0.1328 = 4819

理论上显卡的浮点计算能力FLOPS如何计算？
f指：Shader频率，sm指SM的个数，sp指每个SM包含SP的个数，sf指每个SM包含SFU的个数
统一估算公式： f × sm × ( sp × 2 + sf × 4) = f × sm*sp ×( 2 + sf / sp * 4 ) = f × n × c, c = 2 + sf / sp * 4 ；
粗略估算公式： f × n × 2 ，只算sp的计算能力，忽略sf，sf大约额外贡献1/4~1/2的计算能力。
―― 对于CUDA计算能力1.x，如Tesla架构的G8/G9/GT2系列：	 sf / sp = 2/8  = 1/4 , c = 3 
―― 对于CUDA计算能力2.0，如Fermi架构的GF100系列：	 sf / sp = 4/32 = 1/8 , c = 2.5 
―― 对于CUDA计算能力2.1，如Fermi架构的GF其他系列：	 sf / sp = 8/48 = 1/6 , c = 2.67
―― 对于CUDA计算能力3.0，如Kepler架构的GK104系列：	 sf / sp = 32/192 = 1/6 , c = 2.67
主版本号相同的设备基于相同的核心架构。Tesla架构的主版本号为1，Fermi架构的主版本号为2，Kepler架构的主版本号为3。
参考：
http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units#GeForce_300_Series
http://en.wikipedia.org/wiki/Comparison_of_Nvidia_graphics_processing_units#GeForce_400_Series
CUDA_C_Programming_Guide Section2.5

发散思维：
块数和线程数如何最优化设置？
kernel局部变量对性能有多大影响？
待分出branch进行实验。
基于cuda的gpu算法相对cpu算法，耗费的时间减少了吗？cuda考虑传输时间和不考虑传输时间（模拟数据已经在显卡内存上）这两种情况下，分别加速百分之多少？
理论上显卡的内存带宽是相当大的，NVIDIA的GeForce 8800GTX内存带宽是：86.4GB/s = 1.35G * 128 * 2 * 1/4 ；GTS250带宽：。
峰值带宽计算方法参考：http://dl.acm.org/citation.cfm?id=1345220 （第4节）
