沿用 第三章源码 


Step01-Parallel			第一步-程序的并行化


Step02-Coalesce			第二步-内存的存取模式


Step03-ParallelFurther		第三步-更多的并行化
理论上 256 个 threads 最多只能隐藏 256 cycles 的 latency。但是 GPU 存取 global memory 时的 latency 可能高达 500 cycles 以上。如果增加 thread 数目，就可以看到更好的效率。

Step04-syncthreads		第四步-Thread 的同步
在 CPU 上执行的部份，需要的时间加长了（因为 CPU 现在需要加总 8192 个数字）。为了避免这个问题，我们可以让每个block把自己的每个thread的计算结果进行加总。由于在 GPU 上多做了一些动作，所以它的效率会比较差一些。


Step05-UpgradeFurther		第五步-进一步改善
效率会变差的一个原因是，在这一版的程序中，最后加总的工作，只由每个 block 的 thread 0 来进行，但这并不是最有效率的方法。理论上，把 256 个数字加总的动作，是可以并行化的。最常见的方法，是透过树状的加法。
上一个版本的树状加法是一般的写法，但是它在 GPU 上执行的时候，会有 share memory 的 bank conflict 的问题（详情在后面介绍 GPU 架构时会提到）
如果还要再提高效率，可以把树状加法整个展开。

总结：

本人本实验：
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
				描述		1M数据耗费的时钟周期数		相对上一步提升倍数
优化之前					670M
Step01-Parallel-thread		线程并行	10M				67
Step02-Coalesce-global-memory	显存合并访问	2.5M				1
Step03-Parallel-block		块并行			0.1226M			20
Step04-syncthreads-tree		树状加法		0.1273M			0.96
Step05-UpgradeFurther-tree	conflict 展开循环	0.1260M			1.01	

峰值：Step03-Parallel-block 累积提升：670/0.1226 = 5464

原作者实验：
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
				描述		1M数据耗费的时钟周期数		相对上一步提升倍数
优化之前					640M
Step01-Parallel-thread		线程并行	8.3M				77
Step02-Coalesce-global-memory	显存合并访问	2.6M				3
Step03-Parallel-block		块并行			0.1465M			17
Step04-syncthreads-tree		树状加法		0.1600M			0.92
Step05-UpgradeFurther-tree	conflict 展开循环	0.1328M			1.20	（峰值）

峰值：SStep05-UpgradeFurther-tree 累积提升：640/0.1328 = 4819
